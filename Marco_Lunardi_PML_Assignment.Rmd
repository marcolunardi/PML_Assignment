---
title: "Practical Machine Learning Assignment"
author: "Marco Lunardi"
output: html_document
---


## Summary
The analyzed dataset gathers values from accelerometers on the belt, forearm, arm, and dumbell of 6 people who were asked to perform barbell lifts correctly and incorrectly in 5 different ways.
More information is available from the website here: http://groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset).
The aim of the assignment is to predict the value of the variable `"classe"`, that is the way in which they did the exercise, using the other variables provided with the dataset.
By using a basic Random Forest algorithm it has been possible to get a model able to predict the "classe" value with a good accuracy.


## Data Loading

By using the following R code I downloaded data in R.
Please note that the setting for the na.string setting comes from a preliminary analysis of the raw data in R:

```{r, echo=TRUE,results='hide'}
library(caret)
library(randomForest)
library(rpart)
library(rpart.plot)
library(knitr)
library(corrplot)
fileUrl<-"http://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
download.file(fileUrl,destfile="pml-training.csv")
training <- read.csv("pml-training.csv", header = TRUE, na.strings=c("NA","#DIV/0!",""))
fileUrl<-"http://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
download.file(fileUrl,destfile="pml-testing.csv")
testing <- read.csv("pml-testing.csv", header = TRUE, na.strings=c("NA","#DIV/0!",""))
dim(training)
dim(testing)
```

```{r, echo=TRUE}
dim(training)
dim(testing)
```

As you can see, data are split in two sets: one training set with 19'622 rows, that I'll use to create the model, and a testing set with 20 rows. By the `dim()` functions above, and by the `names()` function below (I omit the results to save space), you can see that the columns number are the same for both the datasets (160), but they differ by 1 column. The variable `"classe"` that we find into the training set, is replaced by the column `"problem_id"` into the testing set, since the testing set gathers the 20 problems (or samples) that the model has to predict.

```{r, echo=TRUE, results='hide'}
names(training)
names(testing)
```


## Data Pre-processing

After having a quick look at the columns into the dataset, I can see that the first 7 columns are pretty useless for our aim.
These 7 columns contain the `"username"` of the people who performed the activities, and the various timestamps related to each sample.
Moreover, by converting all columns to numeric class, and by keeping just the columns with sum not equal to zero, I automatically dump out all columns with `NA` values and keep just complete samples.
I performed also a non-zero-variance check, but it turned out that it isn't required on the reduced dataset I've got.

```{r, echo=TRUE, results='hide'}
traincol <- training[,-(1:7)]
for(i in c(1:(ncol(traincol)-1),1)) {
        traincol[,i] = as.numeric(as.character(traincol[,i]))
}
traincolnames <- colnames(traincol[colSums(is.na(traincol)) == 0])
trainred <- traincol[traincolnames]
trainnzv <- nearZeroVar(trainred, saveMetrics=TRUE)
```

```{r, echo=TRUE}
dim(trainred)
```

So, the reduced dataset `trainred` has 19'622 rows and 53 columns.

Then, I analyzed the correlation of the surviving variables, in order to dump any variable too much correlated with any other.
The following graph shows the correlation between each variable through different color levels.

```{r, echo=TRUE, results='hide',fig.height=9,fig.width=9}
cmatrix <- cor(trainred[,-53])
corrplot(cmatrix, method = "color", type="full", order="original", tl.cex = 0.6,
         tl.col="black", tl.srt = 45)
```

The following R code removes all variables showing a correlation with other variable equal or greater than 0.95; I decided to adopt a selection criteria not too strict, since it's good rule not to dump out too many variables while going through the pre-processing phase.

```{r, echo=TRUE, results='hide'}
traincorr = findCorrelation(cmatrix, cutoff = .95, verbose = FALSE)
train = trainred[,-traincorr]
dim(train)
```

```{r, echo=TRUE}
dim(train)
```

Thanks to the correlation analysis I removed 4 more columns, and now I've got a dataset with 49 columns.

```{r, echo=TRUE}
names(train)
```


## Machine Learning Algorithm: Random Forest

Now I'm ready to use the reduced dataset to build our Machine Learning Algorithm.
Among all algorithms that I tried on the reduced dataset, finally I chose the Random Forest Algorithm, that performed better than the others.
You can see a brief comment about the other algorithms I tried within the conclusions part of this document.
The following R code creates the training and testing set, assigning 2/3 of the data to the training set and 1/3 to the testing set.

```{r, echo=TRUE}
inTrain <- createDataPartition(train$classe, p=(2/3), list=FALSE)
trainset <- train[ inTrain,] 
testset <- train[-inTrain,]
dim(trainset)
dim(testset)
```

Then I built the Random Forest model `trainrf` using the training dataset, and setting numbers of trees to 150, by the following R code:

```{r, echo=TRUE}
set.seed(1)
trainrf=randomForest(classe ~ ., data=trainset, ntree=150, importance=TRUE)
```

The following graph shows the impact of the used variables on the prediction model:

```{r, echo=TRUE,fig.height=8,fig.width=9}
varImpPlot(trainrf, pch=16, color="black")
```


## Cross Validation

To perform the Cross Validation task, I applied the resulting model to the training and testing datasets through the following R code:

```{r, echo=TRUE}
trainpred <- predict(trainrf, trainset, type = "class")
testpred <- predict(trainrf, testset, type = "class")
```


### Cross Validation: In Sample Error Estimate

Through the following R code I can get an estimate of the in-sample error:

```{r, echo=TRUE}
confusionMatrix(trainpred, trainset$classe)
```

As you can see, the model performed a perfect recognition of the training set, with a 1 value for both the Accuracy and K value. So the in-sample error, which is equal to (1-Accuracy), is equal to zero.
When the accuracy is so high on the training set, there is a clear risk of overfitting: the model then could have learnt not just the variable impacts, but also the noise in the data, and that could harm the generalization capabilities of the model itself.
Through the cross validation task, by applying the same model to the testing dataset, it's possible to verify whether there is overfitting into the model or not.


### Cross Validation: Out of Sample Error Estimate

Through the following R code I can get an estimate of the out-of-sample error:

```{r, echo=TRUE}
confusionMatrix(testpred, testset$classe)
```

There is no evident overfitting, since also the accuracy of the testing predictions is very good and so the estimated out of sample error is very low (equal to 1-Accuracy).
Also the K value, showing the "concordance", is quite high.

The efficiency of the resulting model is confirmed by applying it to the 20 samples testing dataset provided for the Prediction Submission part within the Assignment: the model provides the correct outcome for all the proposed 20 samples.

```{r, echo=TRUE}
answers <- predict(trainrf, testing)
answers
```


## Conclusions: Alternative Model and PCA method 

Before going for the Random Forest algorithm I tried the Regression Tree algorithm.
It turns out that the accuracy is significantly worse than that I got by using Random Forest.

```{r, echo=TRUE}
set.seed(1)
traintree <- rpart(classe ~ ., data=trainset, method = "class")
testtree <- predict(traintree, testset, type ="class")
confusionMatrix(testtree, testset$classe)
```

I also tried to pre-process the dataset by using the PCA method to prune the variables before applying the above Random Forest algorithm.
The resulting accuracy is equal to 0.9816, very close but however slightly worse than that of the simple Random Forest model.

So, the simple Random Forest model without PCA pre-processing turns out to be better than the considered alternatives in predicting the `"classe"` values.