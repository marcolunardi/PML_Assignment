---
title: "Practical Machine Learning Assignment"
author: "Marco Lunardi"
output: html_document
---


## Summary
The analyzed dataset gathers values from accelerometers on the belt, forearm, arm, and dumbell of 6 people who were asked to perform barbell lifts correctly and incorrectly in 5 different ways.
More information is available from the website here: http://groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset).
The aim of the assignment is to predict the value of the variable `classe` (which represents the way in which they did the exercise) by using the other variables provided into the dataset.

By applying a basic Random Forest algorithm it has been possible to get a model able to predict the `classe` value with a good accuracy.


## Data Loading

I used the following R code to download the provided training and testing data in R.
Please note that the setting for the `na.string` parameter comes from a preliminary analysis of the raw data in R:

```{r, echo=TRUE,results='hide'}
library(caret)
library(randomForest)
library(rpart)
library(rpart.plot)
library(knitr)
library(corrplot)
fileUrl<-"http://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
download.file(fileUrl,destfile="pml-training.csv")
training <- read.csv("pml-training.csv", header = TRUE, na.strings=c("NA","#DIV/0!",""))
fileUrl<-"http://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
download.file(fileUrl,destfile="pml-testing.csv")
testing <- read.csv("pml-testing.csv", header = TRUE, na.strings=c("NA","#DIV/0!",""))
dim(training)
dim(testing)
```

```{r, echo=TRUE}
dim(training)
dim(testing)
```

As hinted above, the provided data are split in two sets: one training set with 19'622 rows, which has to be used to create the model, and a testing set with 20 rows, which has to be used for the prediction part of the assignment. By the `dim()` functions above, and by the `names()` function below (I omit the results to save space), you can see that the columns number are the same for both the datasets (160), but they differ by 1 column. The variable `classe` that we find into the training set, is replaced by the column `problem_id` into the testing set, since the testing set gathers the 20 problems (or samples) that the model has to predict.

```{r, echo=TRUE, results='hide'}
names(training)
names(testing)
```


## Data Pre-processing

After having a quick look at the columns into the dataset, I can see that the first 7 columns are useless with regard to the assignment aim.
These 7 columns contain the `"username"` of the people who performed the activities, and the various timestamps related to each sample.

Then, by converting all columns to numeric class, and by keeping just the columns with sum not equal to zero, I automatically remove all columns with `NA` values in order to keep just the complete samples.
I performed also a non-zero-variance check, but it turned out that no variable has a too low variance into the `trainred` reduced dataset.

```{r, echo=TRUE, results='hide'}
traincol <- training[,-(1:7)]
for(i in c(1:(ncol(traincol)-1),1)) {
        traincol[,i] = as.numeric(as.character(traincol[,i]))
}
traincolnames <- colnames(traincol[colSums(is.na(traincol)) == 0])
trainred <- traincol[traincolnames]
trainnzv <- nearZeroVar(trainred, saveMetrics=TRUE)
```

```{r, echo=TRUE}
dim(trainred)
```

The reduced dataset `trainred` has now 19'622 rows and 53 columns.

Then, I analyzed the correlation of the surviving variables, in order to remove any variable too much correlated with any other within the dataset.
The following graph shows the correlation between each variable through different color levels.

```{r, echo=TRUE, results='hide',fig.height=9,fig.width=9}
cmatrix <- cor(trainred[,-53])
corrplot(cmatrix, method = "color", type="full", order="original", tl.cex = 0.6,
         tl.col="black", tl.srt = 45)
```

The following R code removes all variables showing an absolute correlation greater than 0.95 with any other variable; I decided to opt for a cutoff value not too strict (0.95), since it's a good general rule not to dump too many variables while going through the pre-processing phase.

```{r, echo=TRUE, results='hide'}
traincorr = findCorrelation(cmatrix, cutoff = .95, verbose = FALSE)
train = trainred[,-traincorr]
dim(train)
```

```{r, echo=TRUE}
dim(train)
```

Thanks to the correlation analysis I removed 4 more columns, and now I've got the `train` dataset with 49 columns.

```{r, echo=TRUE}
names(train)
```


## Machine Learning Algorithm: Random Forest

Now I'm ready to use the reduced `train` dataset to build the Machine Learning Algorithm.
Among all algorithms that I tried on the reduced dataset, I chose the Random Forest Algorithm, which finally performed better than the others.
Please take a look at the conclusions into this document for a brief comment about one more algorithm and one more method I tried on the same dataset.

The following R code creates the training and testing set, assigning 2/3 of the `train` dataset to the training set (`trainset`) and 1/3 to the testing set (`testset`).

```{r, echo=TRUE}
inTrain <- createDataPartition(train$classe, p=(2/3), list=FALSE)
trainset <- train[ inTrain,] 
testset <- train[-inTrain,]
dim(trainset)
dim(testset)
```

Then I built the Random Forest model `trainrf` using the `trainset` dataset, and setting the number of trees to 150, by the following R code:

```{r, echo=TRUE}
set.seed(1)
trainrf=randomForest(classe ~ ., data=trainset, ntree=150, importance=TRUE)
```

The following graph shows the independent variables which have more impact on the prediction model:

```{r, echo=TRUE,fig.height=8,fig.width=9}
varImpPlot(trainrf, pch=16, color="black")
```


## Cross Validation

To perform the Cross Validation task, I applied the resulting `trainrf` model to the `trainset` and `testset` datasets through the following R code:

```{r, echo=TRUE}
trainpred <- predict(trainrf, trainset, type = "class")
testpred <- predict(trainrf, testset, type = "class")
```


### Cross Validation: In Sample Error Estimate

I used the following R code to get an estimate of the in-sample error:

```{r, echo=TRUE}
confusionMatrix(trainpred, trainset$classe)
```

As you can see, the model performed a perfect recognition of the training set, with a **1** value for both the Accuracy and K value. So the in-sample error, which is equal to (1-Accuracy), is equal to **zero**.

When the accuracy is so high on the training set, there is a clear risk of overfitting: the model could have learnt not just the variable impacts on the predicted variable, but also the noise in the data, and that could harm the generalization capabilities of the model itself.
Through the cross validation task (by applying the same model to the testing dataset) it's also possible to verify whether there is some overfitting into the model or not.


### Cross Validation: Out of Sample Error Estimate

Through the following R code I can get an estimate of the out of sample error:

```{r, echo=TRUE}
confusionMatrix(testpred, testset$classe)
```

There is no clear trace of overfitting, since also the accuracy of the testing predictions is very good and equal to **0.9927**; so the estimated out of sample error is very low as well and equal to **0.73%** (=1-Accuracy).
Also the K value, showing the "concordance", is reassuringly high and equal to **0.9907**.

The efficiency of the resulting `trainrf` model is confirmed by applying it to the 20 samples dataset (`testing`) provided for the Prediction Assignment: **the model returns the correct outcome for all the proposed 20 samples**.

```{r, echo=TRUE}
answers <- predict(trainrf, testing)
answers
```


## Conclusions and notes about Tree Model and PCA method

Before going for the Random Forest algorithm I tried a basic Regression Tree algorithm on the same `trainset` dataset.
It turns out that the out of sample accuracy (and so the estimated out of sample error) is significantly worse than that I got by using Random Forest.

```{r, echo=TRUE}
set.seed(1)
traintree <- rpart(classe ~ ., data=trainset, method = "class")
testtree <- predict(traintree, testset, type ="class")
confusionMatrix(testtree, testset$classe)
```

I also tried the PCA method to prune the independent variables within the training dataset (`trainset`) before applying the Random Forest algorithm.
The resulting out of sample accuracy of the Random Forest algorithm with PCA had been equal to **0.9816**, very close but however slightly worse than that of the simple Random Forest model performed above.

So, the basic Random Forest model without PCA pre-processing turns out to be better than the considered alternatives in predicting the `classe` values.